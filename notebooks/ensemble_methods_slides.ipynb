{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Ensemble_methods.jpg\" alt=\"ensemble methods\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Program so far\n",
    "\n",
    "***\n",
    "\n",
    "- Basics of Python\n",
    "- Descriptive and Inferential Statistics\n",
    "- Linear Regression\n",
    "- L1/L2 Regularization\n",
    "- Basic data cleaning and Preprocessing\n",
    "- Feature extraction and Feature engineering\n",
    "- Logistic Regression\n",
    "- Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda\n",
    "***\n",
    "- What is ensembling?\n",
    "- Types of ensembling\n",
    "- Naive aggregation or voting\n",
    "- Bootstrap Aggregating or Bagging\n",
    "- Stacking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Lucius goes to John\n",
    "***\n",
    "Lucius goes on a pilot trip to visit certain universities in Brooklyn. On his way back he decided to pay a visit to John. He got a six-pack of Heineken a rushed to John's place.\n",
    "\n",
    "A much more confident John, played an excellent host to Lucius. While discussing, Lucius discussed a few issues that he was facing. Lucius was through with Decision Trees, but being an Applied Math enthusiast, he was not satisfied with the result. (\"You know how they are!\") He was wondering if anything better could be done.\n",
    "\n",
    "John invited Jay over as he lived only a couple of blocks away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Jay Helps\n",
    "***\n",
    "Jay patiently paid an ear to Lucius's problem, and beamed with excitement, **what's better than one learner? -- multiple learners!**\n",
    "\n",
    "Let's understand more, but before that, let's get over with the routine stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Loan_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4583</td>\n",
       "      <td>1508.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2583</td>\n",
       "      <td>2358.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
       "0             5849                0.0         0.0             360.0   \n",
       "1             4583             1508.0       128.0             360.0   \n",
       "2             3000                0.0        66.0             360.0   \n",
       "3             2583             2358.0       120.0             360.0   \n",
       "4             6000                0.0       141.0             360.0   \n",
       "\n",
       "   Credit_History  Loan_Status  \n",
       "0             1.0            1  \n",
       "1             1.0            0  \n",
       "2             1.0            1  \n",
       "3             1.0            1  \n",
       "4             1.0            1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataframe = pd.read_csv('../data/loan_prediction.csv')\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Split the original data into train & test data, with the column `Loan Status` as the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = dataframe.iloc[:, :-1]\n",
    "y = dataframe.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More is better than one! (1/2)\n",
    "***\n",
    "The idea of **the wisdom of crowds** has been popular since as far back we go, and for a reason. It has been found that the collective intelligence of many often surpasses the intelligence of a single expert.\n",
    "\n",
    "So we don't need to be experts to understand that using multiple learners to make predictions would help!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## More is better than one! (2/2)\n",
    "***\n",
    "\n",
    "Methods of improving model performance by aggregating predictions over several learners are known as **ensemble methods.** Ensembling helps improvise on the stability and predictive power of the model.\n",
    "\n",
    "Ensemble modeling is a powerful way to improve the performance of your model. It usually pays off to apply ensemble learning over and above various models you might be building. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction to Ensemble Methods\n",
    "***\n",
    "Let's explore this technique of Ensembling using a philosophical thought process.\n",
    "\n",
    "#### Condorcet’s Jury Theorem\n",
    "Let's say a jury of voters need to make a decision regarding a binary outcome (for example to convict a defendant or not).\n",
    "\n",
    "If each voter has a probability p of being correct and the probability of a majority of voters being correct is L, then **L > p if p > 0.5** if the voters as independent from each other. Interestingly, **L approaches 1 as the number of voters approaches infinity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction to Ensemble Methods\n",
    "***\n",
    "#### Condorcet’s Jury Theorem\n",
    "\n",
    "In human language, p > 0.5 means that the individual judgments (votes) are at least a little better than random chance.\n",
    "\n",
    "Now, let's take this analogy to the world of ML:\n",
    "\n",
    "* Verdict --> classification prediction\n",
    "* Jury members --> ML models\n",
    "* votes --> individual predictions\n",
    "\n",
    "This means that employing multiple ML models should improve the performance according to the Condorcet's theorem, and it does!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building an intuition (1/2)\n",
    "***\n",
    "Let's say I am interested in buying the new iphone model since I have an extra kidney anyway.\n",
    "\n",
    "However, I am not sure if it is a good model or not. So I ask a bunch of people\n",
    "\n",
    "1. A mobile shopkeeper, whose opinions about a mobile phone model are 80% times correct\n",
    "2. A YouTube gadget reviewer, who is 70% times correct in her opinions about a gadget\n",
    "3. My friend, who is 60% of times correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## When you just get bored from the 'kidney jokes', they do something stupider...\n",
    "***\n",
    "<img src=\"../images/iphone.png\" alt=\"iphone\" style=\"width: 350px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Building an intuition (2/2)\n",
    "***\n",
    "I decide that I will buy the phone if all of them recommend it, and all of them do! In such a case, what is the probability of the new phone turning out to be a bad model?\n",
    "\n",
    "It will be same as the probability of all of them being wrong simultaneously \n",
    "\n",
    ">* P = (1 - 0.8) x (1 - 0.7) x (1 - 0.6)\n",
    ">* P = 0.024\n",
    "\n",
    "Which means that there is 97.6% chance that the phone will be good, given their opinions are independent from each other.\n",
    "\n",
    "Ensemble works on similar principles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "## Strong Learners vs Weak Learners\n",
    "***\n",
    "We discussed how we only need a large number of learners, whose predictive power is just slightly better than random chance (tossing a coin in case of binary classification problem!) for ensembling to work. Such learners have a special name -- \"weak learners\".\n",
    "\n",
    "* **Weak Learner:**\n",
    "Given a labeled dataset, a Weak Learner produces a classifier which is at least a little more accurate than random classification.\n",
    "* **Strong Learner:**\n",
    "We call a machine learning model a Strong Learner which, given a labeled dataset, can produce a classifier to arbitrary accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "## Ensemble of Weak Learner\n",
    "***\n",
    "Given the formulation above, the question we want to ask is this:\n",
    "      \n",
    "**Can an ensemble of weak classifiers produce a single strong classifier?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "## Ensemble of Wise Learners (1/2)\n",
    "***\n",
    "To build ensemble models, we combine multiple models using different methods and hope that the wisdom of the crowd outperforms any individual model. \n",
    "\n",
    "Naturally, not all crowds are wise (for example, greedy investors of a stock market bubble). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "## Ensemble of Wise Learners (2/2)\n",
    "***\n",
    "**Surowiecki** presents a simple framework to evaluate if a given crowd is wise:\n",
    "* **Independence :** Members’ opinions are not determined by the opinions of those around them.\n",
    "* **Diversity of opinion :** Each member should have private information even if it is just an eccentric interpretation of the known facts.\n",
    "* **Decentralization :** Members are able to specialize and draw conclusions based on local knowledge.\n",
    "* **Aggregation :** Some mechanism exists for turning private judgments into a collective decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Surowiecki's framework helps us make sure that the ensemble of ML learners improve overall performance by\n",
    "\n",
    "* Decreasing the variance \n",
    "* Decreasing the bias\n",
    "* Improving the predictive force"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How Ensemble Method Work\n",
    "***\n",
    "Lucius, being in a hurry was keen to jump on how to do things rather than fundamental concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## So How does Ensemble Method Work\n",
    "***\n",
    "Every ensemble algorithm consists of two steps:\n",
    "\n",
    "* Producing a cohort of predictions using simple ML algorithms.\n",
    "* Combining the predictions into one \"aggregated\" model.\n",
    "\n",
    "Ensemble can be achieved through several techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Back to Lucius\n",
    "***\n",
    "As Lucius has a background of applied maths, he has built a strong intuition over the years. Lucius pointed out that the most obvious and intuitive way would be to average out all the possibilities and that would be the final output. Indeed, it was.\n",
    "\n",
    "He was referring to an ensemble method called **Aggregation** or **Voting Ensemble**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "## Naive Aggregation\n",
    "***\n",
    "Naive aggregation works by aggregating the final output through averaging (regression) or voting (classification).\n",
    "\n",
    "- A more sophisticated ensemble might assign weights to the predictions by different learners while aggregating.\n",
    "\n",
    "- Works best with algorithms which learn very differently from each other, thereby complementing each others' decisions\n",
    "\n",
    "**Brain teaser :**\n",
    "When does a voting classifier out do its base learner and when it doesn’t?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "## Soft Voting vs Hard Voting\n",
    "***\n",
    "Since, every classification algorithm first calculates the probabilities of each outcome, and them produces the prediction, the aggregation could be done either on calculated probabilities, or final predictions.\n",
    "\n",
    "* In **hard voting**, the voting classifier takes majority of its base learners’ predictions\n",
    "* In **soft voting**, the voting classifier takes into account the probability values by its base learners \n",
    "\n",
    "In general, Soft voting has been observed to perform better than hard voting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's apply Soft Voting and Hard Voting on the loan prediction dataset. We can pass in a number of classifiers to it. We also check the accuracy by both these methods. We will be using the `VotingClassifier` class in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "decision_clf1 = DecisionTreeClassifier()\n",
    "decision_clf2 = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Hard Voting\n",
    "voting_clf_hard = VotingClassifier(estimators = [('Logistic Regression', log_clf),\n",
    "                                                 ('Decision Tree 1', decision_clf1),\n",
    "                                                 ('Decision Tree 2', decision_clf2)],\n",
    "                                   voting = 'hard')\n",
    "\n",
    "voting_clf_hard.fit(X_train, y_train)\n",
    "y_pred_hard = voting_clf_hard.predict(X_test)\n",
    "accuracy_hard = accuracy_score(y_test, y_pred_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Soft Voting\n",
    "voting_clf_soft = VotingClassifier(estimators = [('Logistic Regression', log_clf),\n",
    "                                                 ('Decision Tree 1', decision_clf1),\n",
    "                                                 ('Decision Tree 2', decision_clf2)],\n",
    "                                   voting = 'soft')\n",
    "voting_clf_soft.fit(X_train, y_train)\n",
    "y_pred_soft = voting_clf_soft.predict(X_test)\n",
    "accuracy_soft = accuracy_score(y_test, y_pred_soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard voting accuracy: 0.6811\n",
      "Soft voting accuracy: 0.6703\n"
     ]
    }
   ],
   "source": [
    "print(\"Hard voting accuracy: %.4f\" %(accuracy_hard))\n",
    "print(\"Soft voting accuracy: %.4f\" %accuracy_soft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The idea of specialists:\n",
    "***\n",
    "Jay was thinking silently the whole time. After a while, he said, **what could better than the wisdom of crowds? --> wisdom of diverse experts!**.\n",
    "\n",
    "This reminded him of the *mantri mandal* of the kings in the older times, where each minister used be an expert of a particular area and the king would ask for opinions from them before taking any major decisions.\n",
    "\n",
    "Unknowingly, Jay was stumbling over a technique called **Bagging**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example (1/2)\n",
    "***\n",
    "Let’s understand this idea better through an example of a multi-speciality hospital. Let’s say the hospital has 3 medical interns which the management can train. Moreover, the hospital deals with 3 types of cases: \n",
    "* Heart diseases\n",
    "* Broken bones\n",
    "* Cancer\n",
    "\n",
    "Now, the management has 2 ways in which they can be trained:\n",
    "* All the interns handle all types of cases \n",
    "* Each intern handle a specific type of case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## We know how interns are..\n",
    "***\n",
    "![hospital](../images/hospital.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example (2/2)\n",
    "***\n",
    "If the management chooses option 1, they will create 3 doctors who can take care of a wide range of cases but since their exposure to each type of case is limited, they will be generalists: MBBS doctors\n",
    "\n",
    "On the other side, if the management lets each intern work on a single category of cases, they can become specialists in their respective areas: A cardiologist, an orthopedist and an oncologist.\n",
    "\n",
    "**Similarly, in bagging, by training each base learner on different sample of data, we make specialist base learners.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "## Bagging\n",
    "***\n",
    "Bagging stands for **B**ootstrap **Agg**regat**ing**.\n",
    "\n",
    "In ensemble algorithms, bagging methods form a class of algorithms which build several instances of a black-box estimator on random subsets of the original training set and then aggregate their individual predictions to form a final prediction. \n",
    "\n",
    "Unlike naive aggregator, bagging uses a single type of base learner. Bagging is a method that involves manipulating the training set by resampling. We learn k base classifiers on k different samples of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The B in Bagging -- Bootstrapping\n",
    "***\n",
    "* The samples are independently created by resampling the training data using uniform weights\n",
    "\n",
    "* This means that the sampling of data points happens with replacement. The process of **sampling with replacement is called Bootstrapping.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bias - Variance trade-off\n",
    "***\n",
    "Because of bootstrapping, each individual predictor has **a higher bias** than if it were trained on the original training set. \n",
    "\n",
    "However, a large number of such biases will cancel each other out when aggregated, hence the bias of the resulting bagging is only slightly higher than a comparable single predictor strong learner. \n",
    "\n",
    "At the same time, because bagging provides a way to reduce overfitting, the variance of resulting strong learner reduced significantly.\n",
    "\n",
    "Generally, the net result is that the ensemble has a similar bias but a lower variance than a single predictor trained on the original training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bagging reduces Variance\n",
    "***\n",
    "![bagging](../images/image22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "## And bagging is practical!\n",
    "***\n",
    "* The learners can all be trained in parallel, via different CPU cores or even different servers. \n",
    "* Similarly, predictions can be made in parallel. \n",
    "* This is one of the reasons why bagging ensembles are such popular methods: **they scale very well.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, lets apply Bagging to the above dataset.\n",
    "We will compare the accuracy of a single learner against an Ensemble of learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "decision_clf = DecisionTreeClassifier()\n",
    "\n",
    "# Fitting single decision tree\n",
    "decision_clf.fit(X_train, y_train)\n",
    "y_pred_decision = decision_clf.predict(X_test)\n",
    "score_dt = accuracy_score(y_test, y_pred_decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Fitting single decision tree\n",
    "log_clf.fit(X_train, y_train)\n",
    "y_pred_decision = log_clf.predict(X_test)\n",
    "score_lr = accuracy_score(y_test, y_pred_decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Fitting bagging classifier with Logisitc Regression\n",
    "bagging_clf1 = BaggingClassifier(LogisticRegression(), n_estimators=100, max_samples=100, \n",
    "                                bootstrap=True, random_state=9)\n",
    "\n",
    "bagging_clf1.fit(X_train, y_train)\n",
    "y_pred_bagging = bagging_clf1.predict(X_test)\n",
    "score_bc_lr = accuracy_score(y_test, y_pred_bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Fitting bagging classifier with Logisitc Regression\n",
    "bagging_clf2 = BaggingClassifier(DecisionTreeClassifier(), n_estimators=100, max_samples=100, \n",
    "                                bootstrap=True, random_state=9)\n",
    "\n",
    "bagging_clf2.fit(X_train, y_train)\n",
    "y_pred_bagging = bagging_clf2.predict(X_test)\n",
    "score_bc_dt = accuracy_score(y_test, y_pred_bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision tree accuracy 0.6865\n",
      "bagging classifier (decision tree) accuracy 0.7243\n",
      "======================================\n",
      "logistic regression accuracy 0.7622\n",
      "bagging classifier (logistic regression) accuracy 0.7676\n"
     ]
    }
   ],
   "source": [
    "print(\"decision tree accuracy %.4f\" %(score_dt))\n",
    "print(\"bagging classifier (decision tree) accuracy %.4f\" %(score_bc_dt))\n",
    "print(\"======================================\")\n",
    "print(\"logistic regression accuracy %.4f\" %(score_lr))\n",
    "print(\"bagging classifier (logistic regression) accuracy %.4f\" %(score_bc_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can easily observe that the accuracy of a BaggingClassifier is much better then that of a single decision tree, but not in case of logistic regression, why? -- more on this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "## Pasting\n",
    "***\n",
    "Just as in bagging we create samples through repeated resampling with replacement, we can create samples **with repeated resampling without replacement** for each base learner. Ensemble on such samples is known as **pasting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bagging vs Pasting (1/2)\n",
    "***\n",
    "* Both bagging and pasting allow training instances to be sampled several times across multiple predictors\n",
    "* But only bagging allows training instances to be sampled several times for the same predictor.\n",
    "* Bootstrapping introduces a bit more diversity in the subsets that each predictor is trained on, so bagging ends up with a slightly higher bias than pasting\n",
    "* This also means that predictors end up being less correlated so the ensemble’s variance is reduced. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bagging vs Pasting (2/2)\n",
    "***\n",
    "* Overall, bagging often results in better models, which explains why it is generally preferred. \n",
    "* However, given spare time and CPU power it is worth using cross- validation to evaluate both bagging and pasting and select the one that works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Out of Bag Evaluation (1/2)\n",
    "***\n",
    "Lucius, being a mathematician, was wondering something about bootstrapping:\n",
    "\n",
    "For bootstrapping with n samples,\n",
    "\n",
    "* The probability of each sample being selected is 1/n\n",
    "* Hence, the probability of not being selected is (1-1/n)\n",
    "* If m such samples are created, then the probability of a sample never being selected is p =  (1-1/n)<sup>m</sup>\n",
    "* When n and m approach large numbers, p ~ e-1 ~ 0.368"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Out of Bag Evaluation (2/2)\n",
    "***\n",
    "\"Aha!\" said, Lucius, \"This means that for each bootstrapping, around 3rd of the original sample will end up not being selected!\"\n",
    "\n",
    "Jay started following the idea: Since a predictor never sees the OOB instances during training, it can be evaluated on these instances, without the need for a separate validation set or cross-validation\n",
    "\n",
    "And since this unselected sample is not in the bag, this validation called **out-of-bag evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similar to the above approach, we just add a new parameter `oob_score` and set it to True, whose default value is False. We also, check out its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72432432432432436"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_of_bag_clf = BaggingClassifier(DecisionTreeClassifier(random_state=9),\n",
    "                                  n_estimators=100,\n",
    "                                  max_samples=100,\n",
    "                                  bootstrap=True,\n",
    "                                  oob_score=True,\n",
    "                                  random_state=9)\n",
    "out_of_bag_clf.fit(X_train, y_train)\n",
    "y_pred = out_of_bag_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Special Mention: The Random Forest\n",
    "***\n",
    "* Random Forest is a bagging algorithm with decision tree as base classifier/regressor\n",
    "* The Random Forest algorithm introduces extra randomness when growing trees \n",
    "* Instead of searching for the very best feature when splitting a node, it searches for the best feature among a random subset of features. \n",
    "* This bootstrapping results in a greater tree diversity, which trades a higher bias for a lower variance, generally yielding an overall better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## John catches up\n",
    "***\n",
    "It took John a bit of time to take in all the \"wisdom\" imparted on it, but once it all settled, John came up with an idea of his own. Let's hear it:\n",
    "\n",
    "So far we have used methods like **averaging** and **voting** to aggregate the predictions by all the base learners, thereby we have assigned same weights to the predictions made by the base learners.\n",
    "\n",
    "However, it might be possible that some base learners might be better at predicting than the others. So, a better aggregation scheme could be to assign some kind of weights to the predictions made by base learners!\n",
    "\n",
    "And since we have been learning machine learning to predict things anyway,... you see where we are going with this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "## Stacking\n",
    "***\n",
    "\n",
    "Stacking is an ensemble learning technique to combine multiple classification models via a meta-classifier\n",
    "It is based on a simple idea: instead of using trivial functions to aggregate the predictions of all predictors in an ensemble, we train a model to perform this aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "## Stacking\n",
    "***\n",
    "\n",
    "![stacking](../images/image23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How Stacking works?\n",
    "***\n",
    "* First, the training set is split in two subsets. \n",
    "* The first subset is used to train the predictors in the first layer\n",
    "* Next, the first layer predictors are used to make predictions on the second (held-out) set\n",
    "* This ensures that the predictions are “clean,” since the predictors never saw these instances during training.\n",
    "* The meta-classifier is trained on this new training set, so it learns to predict the target value given the first layer’s predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "## Stacking with `mlxtend`\n",
    "***\n",
    "Sadly, stacking is not implemented in sklearn.\n",
    "\n",
    "Scikit-Learn does not support stacking directly, luckily but it is not very hard to create your own stacking ensemble. Alternatively a python library called mlextend supports stacking and has very similar api as sklearn!\n",
    "\n",
    "Check out the link for `Stacking` implementation in a python library called **[mlxtend](https://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/)**.\n",
    "\n",
    "You can install it using the `pip` command: `pip install mlxtend`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "decision_tree_clf = DecisionTreeClassifier() \n",
    "models = [log_clf, decision_tree_clf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68108108108108112"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacking_clf = StackingClassifier(classifiers = models,\n",
    "                                 meta_classifier = decision_tree_clf)\n",
    "\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "y_pred = stacking_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Recap.png\" alt=\"Recap\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "# In-session Recap Time\n",
    "***\n",
    "* Ensembling\n",
    "* Strong and Weak Learners\n",
    "* Ensemble of Wise Learners\n",
    "* Naive Aggregation\n",
    "* Soft Voting & Hard Voting\n",
    "* Bagging\n",
    "* Pasting\n",
    "* Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thank You\n",
    "***\n",
    "### Next Session: xgboost\n",
    "For more queries - Reach out to academics@greyatom.com "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
